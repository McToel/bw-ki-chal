{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finde ein Model welches den Trainingsdatensatz perfekt lernt (Accuracy = 1.). Im Notebook kannst du dir für das Model auch die Testgenauigkeit ausgeben lassen.\n",
    "\n",
    "In dieser Aufgabe siehst du wie man Overfitting erkennen kann: das Model ist auf die Trainingsdaten überangepasst und generalisiert\n",
    "nicht auf anderen Daten. Der Konstante Klassifizierer wiederum ist ein Beispiel für Underfitting: das Model ist sowohl auf den Trainingsdaten als auch auf neuen Daten gleich schlecht. Die Schwierigkeit besteht darin ein Modell zu finden das weder underfittet noch overfittet. In der Challenge ist es deine Aufgabe ein Model zu finden, dass möglichst gut generalisiert. (Die Challenge werden wir in den nächsten Tagen freischalten.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "\n",
    "D = np.load('data/train_data.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(D):\n",
    "\n",
    "    train, test = D[:200], D[200:]  # Die Auswertung der Aufgabe basiert auf diesem Split\n",
    "\n",
    "    # Hyper-parameter\n",
    "    n_steps = ...\n",
    "    learning_rate = ...\n",
    "    input_size = 13\n",
    "    output_size = 1\n",
    "\n",
    "    # Trainings-Daten vorbereiten\n",
    "    X = train[:, :-1].astype(np.float32)\n",
    "    y = train[:, -1].astype(np.float32)\n",
    "    X_train = torch.from_numpy(X)\n",
    "    y_train = torch.from_numpy(y)\n",
    "    feature_means = torch.mean(X_train, dim=0)\n",
    "\n",
    "    # Test-Daten vorbereiten\n",
    "    X = test[:, :-1].astype(np.float32)\n",
    "    y = test[:, -1].astype(np.float32)\n",
    "    X_test = torch.from_numpy(X)\n",
    "    y_test = torch.from_numpy(y)\n",
    "\n",
    "    # Modell definieren\n",
    "    class MLP(nn.Module):\n",
    "        def __init__(self, input_size):\n",
    "            super().__init__()\n",
    "            self.layers = nn.Sequential(\n",
    "                ...  # Definiere hier dein eigenes Model\n",
    "            )\n",
    "\n",
    "        def forward(self, x):\n",
    "            x = x - feature_means\n",
    "            out = self.layers(x)\n",
    "            return out\n",
    "\n",
    "    model = MLP(input_size)\n",
    "\n",
    "    # loss and optimizer\n",
    "    # checkout: https://pytorch.org/docs/stable/nn.html#torch.nn.BCEWithLogitsLoss\n",
    "    criterion = nn.BCEWithLogitsLoss()  # sigmoid + binary cross entropy\n",
    "\n",
    "    # optimizer\n",
    "    # Dokumentation: https://pytorch.org/docs/stable/optim.html#torch.optim.Adam\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    # Alternative zu Adam\n",
    "    # Dokumentation: https://pytorch.org/docs/stable/optim.html#torch.optim.SGD\n",
    "    # momentum = 0.9  # Wert zwischen 0. und 1.\n",
    "    # optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum)\n",
    "\n",
    "    # trainieren des Modells\n",
    "    for e in range(n_steps):\n",
    "        # forward pass\n",
    "        outputs = model.forward(X_train)[:, 0]  # Xw (linear layer)\n",
    "        loss = criterion(outputs, y_train)  # sigmoid and cross-entropy loss\n",
    "\n",
    "        # backward pass (automatically computes gradients)\n",
    "        optimizer.zero_grad()  # reset gradients (torch accumulates them)\n",
    "        loss.backward()  # computes gradients\n",
    "\n",
    "        # Optimierungsschritt durchfuehren\n",
    "        optimizer.step()\n",
    "\n",
    "        # berechne Trainings-Accuracy\n",
    "        outputs = model.forward(X_train)[:, 0]\n",
    "        pred_y = outputs > 0\n",
    "        is_correct = torch.eq(pred_y, y_train.byte()).float()\n",
    "        accuracy_train = torch.mean(is_correct).item()\n",
    "\n",
    "        # berechne Test-Accuracy\n",
    "        outputs = model.forward(X_test)[:, 0]\n",
    "        pred_y = outputs > 0\n",
    "        is_correct = torch.eq(pred_y, y_test.byte()).float()\n",
    "        accuracy_test = torch.mean(is_correct).item()\n",
    "\n",
    "    print(f'Epoch {e}, Loss: {loss:.4f}, Acc train: {accuracy_train:.2f},' \\\n",
    "          f'Acc test: {accuracy_test:.2f}')\n",
    "    return model\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
