{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Das trainieren von komplizierten Modellen geht dank Pytorch genaus so einfach. Unten ist der Code für ein sogenanntes Multi-Layer-Perceptron (MLP) - ein [mehrlagiges neuronales Netzwerk](https://de.wikipedia.org/wiki/Perzeptron).\n",
    "\n",
    "Ein MLP-Layer besteht aus einer linearen Funktion (in Pytorch `nn.Linear`) und einer nicht-linearen Funktion (zum Beispiel ReLU in Pytorch `nn.ReLU`). Die ReLU Funktion verändert positive Werte nicht d. h. \\\\(r(x)=x\\\\). Alle negative Werte jedoch werden auf die 0 Abgebi\\\n",
    "ldet d.h. \\\\(r(x)=0\\\\). Die Definition ist: \\\\(r(x) = \\max(0, x)\\\\).\n",
    "\n",
    "Allgemein gilt, dass tiefe Modelle flexibler sind und sich besser an die Daten anpassen. Man kann mit solchen Netzwerken sehr komplizierte Probleme lösen. Zum Beispiel können wir mit ähnlichen Netzwerken Bilder von Hunden, den darauf abgebildeten Hundearten zuordnen. Wi\\\n",
    "r müssen jedoch bedenken, dass wir die Netzwerke vorher trainieren müssen und dafür benötigen wir viele Beispielbilder inklusive den dazugehörigen Labels. [Video zu neuronalen Netzwerken](https://www.youtube.com/watch?v=o3RDCSJH2oo)\n",
    "\n",
    "Finde auch hier passende Werte für die sogenannten Hyperparameter\n",
    "`n_steps` und `learning_rate`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "\n",
    "D = np.load('data/train_data.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(D):\n",
    "\n",
    "    # Hyper-parameter\n",
    "    n_steps = ...\n",
    "    learning_rate = ...\n",
    "    input_size = 13\n",
    "    output_size = 1\n",
    "\n",
    "    # Daten vorbereiten\n",
    "    X = D[:, :-1].astype(np.float32)\n",
    "    y = D[:, -1].astype(np.float32)\n",
    "    X = torch.from_numpy(X)\n",
    "    y = torch.from_numpy(y)\n",
    "    feature_means = torch.mean(X, dim=0)\n",
    "\n",
    "    # Modell definieren\n",
    "    class MLP(nn.Module):\n",
    "        def __init__(self, input_size):\n",
    "            super().__init__()\n",
    "            n_neurons = 4\n",
    "            self.layers = nn.Sequential(\n",
    "                nn.Linear(input_size, n_neurons),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(n_neurons, n_neurons),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(n_neurons, output_size)\n",
    "            )\n",
    "\n",
    "        def forward(self, x):\n",
    "            x = x - feature_means\n",
    "            out = self.layers(x)\n",
    "            return out\n",
    "\n",
    "    model = MLP(input_size)\n",
    "\n",
    "    # loss\n",
    "    # Dokumentation: https://pytorch.org/docs/stable/nn.html#torch.nn.BCEWithLogitsLoss\n",
    "    criterion = nn.BCEWithLogitsLoss()  # Sigmoid und Binary-Cross-Entropy-Loss\n",
    "\n",
    "    # optimizer\n",
    "    # Dokumentation: https://pytorch.org/docs/stable/optim.html#torch.optim.Adam\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    # Alternative zu Adam\n",
    "    # Dokumentation: https://pytorch.org/docs/stable/optim.html#torch.optim.SGD\n",
    "    # momentum = 0.9  # Wert zwischen 0. und 1.\n",
    "    # optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum)\n",
    "\n",
    "    # trainieren des Modells\n",
    "    for e in range(n_steps):\n",
    "        # forward pass\n",
    "        outputs = model.forward(X)[:, 0]\n",
    "        cost = criterion(outputs, y)\n",
    "\n",
    "        # backward pass (berechnet die Gradienten automatisch)\n",
    "        optimizer.zero_grad()  # reset gradients (torch akkumuliert Gradienten)\n",
    "        cost.backward()  # berechnen der Gradienten\n",
    "\n",
    "        # Optimierungsschritt durchfuehren\n",
    "        optimizer.step()\n",
    "\n",
    "        # Berechnung der Accuracy\n",
    "        pred_labels = outputs > 0\n",
    "        is_correct = torch.eq(pred_labels, y.byte()).float()\n",
    "        accuracy = torch.mean(is_correct).item()\n",
    "\n",
    "    return model\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
