{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bis jetzt haben wir die Gradienten der Kostenfunktion noch selbst hergeleitet und anschliessend implementiert um die Optimierung durchzuführen. Diese Schritte lassen sich zum Glück auch sehr einfach automatisieren, zum Beispiel mit Pytorch.\n",
    "\n",
    "Wir haben unten das Logistic-Regression Beispiel der vorangegangenen Aufgaben in Pytorch implementiert. Das Modell, d.h. die lineare Klassifikationsfunktion, wird durch `nn.Linear` realisiert. Die Kostenfunktion ist in `nn.BCEWithLogitsLoss` implementiert. Noch besser: \\\n",
    "die Ableitung wird automatisch von Pytorch mit der Funktion `backward` berechnet. Ihr braucht den Gradienten also nicht mehr per Hand berechnen. Ein Optimierungsschritt, d.h. die Anpassung der Modellparameter in Richtung des (negativen) Gradienten, wird mit der Funktion\\\n",
    " `step` durchgeführt.\n",
    "\n",
    "Neben dem normalen Gradientabstieg können wir nun auch verwandte Abstiegsverfahren nutzen. Diese Verfahren heißen zum Beispiel `Adam` oder `RMSprop` und modifizieren den Gradientabstieg so, dass das Minimum der Kostenfunktion teilweise deutlich schneller und zuverlässig\\\n",
    "er gefunden werden kann.\n",
    "\n",
    "Aufgabe: Finde passende Werte für die sogenannten Hyperparameter `n_steps` und `learning_rate`.\n",
    "\n",
    "Hier findest du Video-Tutorials zu Pytorch:\n",
    "[Das erste eigene Neuronale Netz](https://www.youtube.com/watch?v=GBzojftwfGQ)\n",
    "[Das Netz füttern](https://www.youtube.com/watch?v=u6V69py5Aps)\n",
    "[Das Netz lernen lassen](https://www.youtube.com/watch?v=F4V2617urQs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "\n",
    "D = np.load('data/train_data.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(D):\n",
    "\n",
    "    # Hyper-parameter\n",
    "    n_steps = ...\n",
    "    learning_rate = ...\n",
    "    input_size = 13  # Anzahl Merkmale (features)\n",
    "    output_size = 1\n",
    "    \n",
    "    # Daten vorbereiten\n",
    "    X = D[:, :-1].astype(np.float32)\n",
    "    labels = D[:, -1].astype(np.float32)\n",
    "    X = torch.from_numpy(X)\n",
    "    y = torch.from_numpy(labels)\n",
    "\n",
    "    # Modell definieren\n",
    "    class LogisticRegression(nn.Module):\n",
    "        def __init__(self, input_size):\n",
    "            super().__init__()\n",
    "            self.linear = nn.Linear(input_size, output_size)  # Xw (linear layer)\n",
    "\n",
    "        def forward(self, x):\n",
    "            out = self.linear(x)\n",
    "            return out\n",
    "    model = LogisticRegression(input_size)\n",
    "\n",
    "    # loss\n",
    "    # Dokumentation: https://pytorch.org/docs/stable/nn.html#torch.nn.BCEWithLogitsLoss\n",
    "    criterion = nn.BCEWithLogitsLoss()  # Sigmoid und Binary-Cross-Entropy-Loss\n",
    "\n",
    "    # optimizer\n",
    "    # Dokumentation: https://pytorch.org/docs/stable/optim.html#torch.optim.Adam\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    # Alternative zu Adam\n",
    "    # Dokumentation: https://pytorch.org/docs/stable/optim.html#torch.optim.SGD\n",
    "    # momentum = 0.9  # Wert zwischen 0. und 1.\n",
    "    # optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum)\n",
    "\n",
    "    # trainieren des Modells\n",
    "    for e in range(n_steps):\n",
    "        # forward pass\n",
    "        outputs = model.forward(X)[:, 0]\n",
    "        cost = criterion(outputs, y)\n",
    "\n",
    "        # backward pass (berechnet die Gradienten automatisch)\n",
    "        optimizer.zero_grad()  # reset gradients (torch akkumuliert Gradienten)\n",
    "        cost.backward()  # berechnen der Gradienten\n",
    "\n",
    "        # Optimierungsschritt durchfuehren\n",
    "        optimizer.step()\n",
    "\n",
    "        # Berechnung der Accuracy\n",
    "        pred_labels = outputs > 0\n",
    "        is_correct = torch.eq(pred_labels, y.byte()).float()\n",
    "        accuracy = torch.mean(is_correct).item()\n",
    "\n",
    "    return model\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
